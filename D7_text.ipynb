{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47826932cbc70e24159d871c4417febc",
     "grade": false,
     "grade_id": "cell-b7d3dafa230af1af",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Text Analysis\n",
    "\n",
    "In this lab we'll do a short text analysis so that you start to become familiar with the packages and tools available to you in Python to work with text data. Nothing here will be very in-depth - it's supposed to be able to be completed in a short period of time after all. But, it will give you a starting point for your final assignment and projects, should you want to analyze text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89c80e12d3603ed2368444130a4c42d1",
     "grade": false,
     "grade_id": "cell-ab7cad2d5d781d48",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Data Science Question\n",
    "In this short project, we're going to answer the question: *For each presidential inauguration, which word is most unique?* \n",
    "\n",
    "To do this, we'll use the text from each Inaugural address in American history and carry out a TF-IDF analaysis.\n",
    "\n",
    "Secondarily, we'll think about whether these words make sense in the context of the history at the time and visualize words uniqueness over the course of history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3f5a3452b32a202b1702217a52888f7",
     "grade": false,
     "grade_id": "cell-a6def326b354bf91",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Part I : Setup & Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e7f016a65a560ae35fd9375f3b58cd8",
     "grade": false,
     "grade_id": "cell-0adbfeb986a31274",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "This lab uses a number of different functions across multiple packages. **Run the following code cell and take a look at each package we'll be using below. Make sure you understand what the package is used for. Be sure to familiarize yourself with anything that you're not yet familiar with.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:22.902355Z",
     "iopub.status.busy": "2026-02-19T01:23:22.902355Z",
     "iopub.status.idle": "2026-02-19T01:23:24.724538Z",
     "shell.execute_reply": "2026-02-19T01:23:24.724035Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import nltk package \n",
    "# NLTK provides support for a wide variety of text processing tasks: \n",
    "# tokenization, stemming, proper name identification, part of speech identification, etc. \n",
    "#   PennTreeBank word tokenizer \n",
    "#   English language stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# scikit-learn imports\n",
    "#   TF-IDF Vectorizer that first removes widely used words in the dataset and then transforms test data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# import re for regular expression\n",
    "import re\n",
    "\n",
    "## seaborn for plotting\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.2, style=\"white\")\n",
    "\n",
    "# import matplotlib for plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# set plotting size parameter\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "# improve resolution\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2cb6b2e981933ee13c63fc87ff19a932",
     "grade": false,
     "grade_id": "cell-db67b6ab3ff028c6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "To get started on your text analysis using the `nltk` package, run the code below to **download the NLTK English tokenizer ('punkt'), stopwords of all languages ('stopwords') from `nltk`, and the inaugural dataset from `nltk` ('inaugural')**. To determine what code you'll need to do this, you can explore the `download` method [here](https://www.nltk.org/) or their book [here](http://www.nltk.org/book/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:24.726556Z",
     "iopub.status.busy": "2026-02-19T01:23:24.726047Z",
     "iopub.status.idle": "2026-02-19T01:23:24.878990Z",
     "shell.execute_reply": "2026-02-19T01:23:24.878990Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fb7cb7e75ebe1e3216bf263886dac4e",
     "grade": false,
     "grade_id": "cell-ad5ce2ec5ce7d4af",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rabwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rabwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\rabwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba26ea06f76878333be34a20d314ae79",
     "grade": false,
     "grade_id": "cell-7443ebf7f9e7dc56",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that you have downloaded a few of the datasets you'll need, **import the `inaugural` dataset from `nltk.corpus`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:24.880499Z",
     "iopub.status.busy": "2026-02-19T01:23:24.879995Z",
     "iopub.status.idle": "2026-02-19T01:23:24.882523Z",
     "shell.execute_reply": "2026-02-19T01:23:24.882523Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cac178fd8d02f790ee8659bf8c7d2f5",
     "grade": false,
     "grade_id": "cell-0c25d2300264c1f1",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:24.884033Z",
     "iopub.status.busy": "2026-02-19T01:23:24.884033Z",
     "iopub.status.idle": "2026-02-19T01:23:24.886592Z",
     "shell.execute_reply": "2026-02-19T01:23:24.886592Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "176838b3962a7ee0d1dc818950fa6ef6",
     "grade": true,
     "grade_id": "cell-df848beec787864c",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "030162773d233d795cbf9c87f459f612",
     "grade": false,
     "grade_id": "cell-f3ffc530ee1a4a55",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "If all is working well, the following cell should display the files included in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:24.888596Z",
     "iopub.status.busy": "2026-02-19T01:23:24.887597Z",
     "iopub.status.idle": "2026-02-19T01:23:24.892619Z",
     "shell.execute_reply": "2026-02-19T01:23:24.892619Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfc1dd2765d986827c01616b3802ba98",
     "grade": false,
     "grade_id": "cell-fd6127723bcdc17d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt',\n",
       " '2013-Obama.txt',\n",
       " '2017-Trump.txt',\n",
       " '2021-Biden.txt',\n",
       " '2025-Trump.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8f7b5552b9d13fa79063c8c4594e834",
     "grade": false,
     "grade_id": "cell-91f276f62e3d3658",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see there is one file from each address. And, you'll note that the filename includes the year of each address. We'll want to use that address later, so **write code that extracts each year from the filename and stores it as a list. Assign this list to the variable `years`.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:24.893130Z",
     "iopub.status.busy": "2026-02-19T01:23:24.893130Z",
     "iopub.status.idle": "2026-02-19T01:23:24.896899Z",
     "shell.execute_reply": "2026-02-19T01:23:24.896899Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ffcbe5da4dcac26fbf55b0396de2f13",
     "grade": false,
     "grade_id": "cell-f8c35b3a8d165962",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "years = [fileid[:4] for fileid in inaugural.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:24.898507Z",
     "iopub.status.busy": "2026-02-19T01:23:24.897949Z",
     "iopub.status.idle": "2026-02-19T01:23:24.901045Z",
     "shell.execute_reply": "2026-02-19T01:23:24.900530Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "119d43656e4032eed0749e08fe236b25",
     "grade": true,
     "grade_id": "cell-dba761f98cf172ae",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(years) == len(inaugural.fileids())\n",
    "assert years[1] == '1793'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7edb305c1c6f07e44e9ca42c13b2b171",
     "grade": false,
     "grade_id": "cell-76be36ec8ebfd3be",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's take a look at one of these addresses. We'll pick a short one - Washington's *second* address. **Run the code below to take a look.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:24.901569Z",
     "iopub.status.busy": "2026-02-19T01:23:24.901569Z",
     "iopub.status.idle": "2026-02-19T01:23:24.905096Z",
     "shell.execute_reply": "2026-02-19T01:23:24.905096Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e7541287e1b33c11fbb018814bdec56",
     "grade": false,
     "grade_id": "cell-cdd5c90b100ac14c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fellow citizens, I am again called upon by the voice of my country to execute the functions of its Chief Magistrate. When the occasion proper for it shall arrive, I shall endeavor to express the high sense I entertain of this distinguished honor, and of the confidence which has been reposed in me by the people of united America.\\n\\nPrevious to the execution of any official act of the President the Constitution requires an oath of office. This oath I am now about to take, and in your presence: That if it shall be found during my administration of the Government I have in any instance violated willingly or knowingly the injunctions thereof, I may (besides incurring constitutional punishment) be subject to the upbraidings of all who are now witnesses of the present solemn ceremony.\\n\\n \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see Washington's Second Inaugural Address\n",
    "inaugural.raw('1793-Washington.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6042f47ffb775dc91ddecfd484d331d3",
     "grade": false,
     "grade_id": "cell-e10ca8d112c1c216",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You'll notice that there are some new line characters, as well as a colon, some commas, some periods. We're really only interested in the words though for TF-IDF, so let's remove all punctuation. **Write code that returns a list (`text`), where each element in the list includes the text as above, but with:\n",
    "- punctuation removed \n",
    "- each word separated by a space\n",
    "- all words are lower case (i.e. \"Constitution\" should be \"constitution)\n",
    "\n",
    "Assign this to the variable `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:24.905607Z",
     "iopub.status.busy": "2026-02-19T01:23:24.905607Z",
     "iopub.status.idle": "2026-02-19T01:23:24.930336Z",
     "shell.execute_reply": "2026-02-19T01:23:24.929822Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a493095921512a025973b3644fce808",
     "grade": false,
     "grade_id": "cell-5a2c6d6fc38b0acd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "text = [re.sub(r'[^A-Za-z0-9]+', ' ', x) for x in [inaugural.raw(file_id) for file_id in inaugural.fileids()]]\n",
    "text = list(map(str.lower, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:24.931375Z",
     "iopub.status.busy": "2026-02-19T01:23:24.931375Z",
     "iopub.status.idle": "2026-02-19T01:23:25.265065Z",
     "shell.execute_reply": "2026-02-19T01:23:25.265065Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "504c353c2af86913fe3b937bd371370a",
     "grade": true,
     "grade_id": "cell-dc70d907b5789bed",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) == \u001b[32m59\u001b[39m\n\u001b[32m      3\u001b[39m out = re.search(\u001b[33m'\u001b[39m\u001b[33m^fellow\u001b[39m\u001b[33m'\u001b[39m,text[\u001b[32m0\u001b[39m])\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m out != \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "assert isinstance(text, list)\n",
    "assert len(text) == 59\n",
    "out = re.search('^fellow',text[0])\n",
    "assert out != None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "319b513aed8c51e5673a5a04db627cdb",
     "grade": false,
     "grade_id": "cell-1154448bde27c653",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "If you've done this correctly and you **run the following cell, all punctuation should be stripped from the text, so that you only see the words from Washington's second address, separated by spaces, with all words lowercase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.266071Z",
     "iopub.status.busy": "2026-02-19T01:23:25.266071Z",
     "iopub.status.idle": "2026-02-19T01:23:25.270139Z",
     "shell.execute_reply": "2026-02-19T01:23:25.269597Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8aeb5320f83549d0eef01852653843af",
     "grade": false,
     "grade_id": "cell-18bd2aae55ec3725",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fellow citizens i am again called upon by the voice of my country to execute the functions of its chief magistrate when the occasion proper for it shall arrive i shall endeavor to express the high sense i entertain of this distinguished honor and of the confidence which has been reposed in me by the people of united america previous to the execution of any official act of the president the constitution requires an oath of office this oath i am now about to take and in your presence that if it shall be found during my administration of the government i have in any instance violated willingly or knowingly the injunctions thereof i may besides incurring constitutional punishment be subject to the upbraidings of all who are now witnesses of the present solemn ceremony '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2c9f35a613fde13d7f0026b5b31774e",
     "grade": false,
     "grade_id": "cell-66f13e717de0d1c7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "With that, you now have a dataset ready for analysis by TF-IDF!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18e71861286c19036538389364986148",
     "grade": false,
     "grade_id": "cell-6bf8534e57d52fce",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Part II : Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baa1e8ca7438e3fcb7b3d3634f42af4b",
     "grade": false,
     "grade_id": "cell-fdffd7def7829489",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "To get started on your TF-IDF analysis, you'll first want to **create a `TfidfVectorizer` object to transform your text data into vectors. Assign this `TfidfVectorizer` object to `tfidf`.**\n",
    "\n",
    "In this object, you'll need to **pass five arguments to initialize `tfidf`**: \n",
    "* set to apply TF scaling: `sublinear_tf=True`\n",
    "* analyze at the word-level: `analyzer='word'`\n",
    "* set maximum number of unique words: ` max_features=2000`\n",
    "* specify that you want to tokenize the data using the word_tokenizer from NLTK: `tokenizer=word_tokenize`\n",
    "* remove English language stop words: `stop_words=stopwords.words(\"english\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.270654Z",
     "iopub.status.busy": "2026-02-19T01:23:25.270654Z",
     "iopub.status.idle": "2026-02-19T01:23:25.359131Z",
     "shell.execute_reply": "2026-02-19T01:23:25.358617Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37e81157cd42d149551f5c2a2dcf3eff",
     "grade": false,
     "grade_id": "cell-33b91ae24ac3ec7d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\rabwa/nltk_data'\n    - 'C:\\\\Users\\\\rabwa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'C:\\\\Users\\\\rabwa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\rabwa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\rabwa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m stop_words_processed = \u001b[38;5;28mset\u001b[39m(\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43menglish\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      3\u001b[39m tfidf = TfidfVectorizer(sublinear_tf=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      4\u001b[39m                         analyzer=\u001b[33m'\u001b[39m\u001b[33mword\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m                         max_features=\u001b[32m2000\u001b[39m,\n\u001b[32m      6\u001b[39m                         tokenizer=word_tokenize,\n\u001b[32m      7\u001b[39m                         stop_words=stop_words_processed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\rabwa/nltk_data'\n    - 'C:\\\\Users\\\\rabwa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'C:\\\\Users\\\\rabwa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\rabwa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\rabwa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "stop_words_processed = set(word_tokenize(' '.join(stopwords.words('english'))))\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True,\n",
    "                        analyzer='word',\n",
    "                        max_features=2000,\n",
    "                        tokenizer=word_tokenize,\n",
    "                        stop_words=stop_words_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.360702Z",
     "iopub.status.busy": "2026-02-19T01:23:25.360184Z",
     "iopub.status.idle": "2026-02-19T01:23:25.371973Z",
     "shell.execute_reply": "2026-02-19T01:23:25.371973Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "940102fafaa436016e050ed6291ef8eb",
     "grade": true,
     "grade_id": "cell-2fd89e1e4135ac38",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mtfidf\u001b[49m.analyzer == \u001b[33m'\u001b[39m\u001b[33mword\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m tfidf.max_features == \u001b[32m2000\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m tfidf.tokenizer == word_tokenize\n",
      "\u001b[31mNameError\u001b[39m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "assert tfidf.analyzer == 'word'\n",
    "assert tfidf.max_features == 2000\n",
    "assert tfidf.tokenizer == word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09b7c3de95aa37fc9d0df921039249cc",
     "grade": false,
     "grade_id": "cell-f71715b9480eb1f5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now, it's time to calculate TF-IDF for words across our corpus of Inaugural addresses! \n",
    "\n",
    "To do this:\n",
    "\n",
    "1. generate a DataFrame `inaug_tfidf` using the `tfidf.fit_transform` function to calculate TF-IDF on your `text` variable. \n",
    "2. Be sure that your index here is the year of the address and the columns are named with the columns of the words the values represent. The `get_feature_names` method from `tfidf` may help you accomplish the columns name assignment. And the `years` you created earlier may help you with the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.372484Z",
     "iopub.status.busy": "2026-02-19T01:23:25.372484Z",
     "iopub.status.idle": "2026-02-19T01:23:25.397847Z",
     "shell.execute_reply": "2026-02-19T01:23:25.397847Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ead71e64807634a8f3ad1ab22577af95",
     "grade": false,
     "grade_id": "cell-02461e46f376ab9b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m texts = [inaugural.raw(file_id) \u001b[38;5;28;01mfor\u001b[39;00m file_id \u001b[38;5;129;01min\u001b[39;00m inaugural.fileids()]\n\u001b[32m      3\u001b[39m cleaned_texts = [re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text).lower() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tfidf_matrix = \u001b[43mtfidf\u001b[49m.fit_transform(cleaned_texts)\n\u001b[32m      5\u001b[39m inaug_tfidf = pd.DataFrame(tfidf_matrix.toarray(), index=years, columns=tfidf.get_feature_names())\n",
      "\u001b[31mNameError\u001b[39m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "texts = [inaugural.raw(file_id) for file_id in inaugural.fileids()]\n",
    "cleaned_texts = [re.sub(r'[^\\w\\s]', '', text).lower() for text in texts]\n",
    "tfidf_matrix = tfidf.fit_transform(cleaned_texts)\n",
    "inaug_tfidf = pd.DataFrame(tfidf_matrix.toarray(), index=years, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.397847Z",
     "iopub.status.busy": "2026-02-19T01:23:25.397847Z",
     "iopub.status.idle": "2026-02-19T01:23:25.409961Z",
     "shell.execute_reply": "2026-02-19T01:23:25.409961Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d13bf3f3d204ed1fc8c6019a2c9c360",
     "grade": true,
     "grade_id": "cell-3b24052773747528",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inaug_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43minaug_tfidf\u001b[49m.index) == \u001b[38;5;28mlen\u001b[39m(years)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inaug_tfidf.columns) == \u001b[32m2000\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m inaug_tfidf.shape == (\u001b[32m59\u001b[39m, \u001b[32m2000\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'inaug_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "assert len(inaug_tfidf.index) == len(years)\n",
    "assert len(inaug_tfidf.columns) == 2000\n",
    "assert inaug_tfidf.shape == (59, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71091dc37136f4ace799aa7908167e38",
     "grade": false,
     "grade_id": "cell-ac1707808d9891ad",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Part 3: Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cbe99824b024979d9b08cedfffa065a",
     "grade": false,
     "grade_id": "cell-29590e1aff297b9f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We're almost there. We now have a DataFrame that includes the TF-IDF for the top 2000 words in our corpus! **Now, you'll want to extract the single most unique word from each address. Assign this information (most likely a Series object) to the variable `most_unique`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.411469Z",
     "iopub.status.busy": "2026-02-19T01:23:25.410965Z",
     "iopub.status.idle": "2026-02-19T01:23:25.422260Z",
     "shell.execute_reply": "2026-02-19T01:23:25.421748Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3474a31bba39cf569b1f179e347fe91",
     "grade": false,
     "grade_id": "cell-85c21b394a6f9ea1",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inaug_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Extract the most unique word from each address\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m most_unique = \u001b[43minaug_tfidf\u001b[49m.idxmax(axis = \u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m most_unique\n",
      "\u001b[31mNameError\u001b[39m: name 'inaug_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Extract the most unique word from each address\n",
    "most_unique = inaug_tfidf.idxmax(axis = 1)\n",
    "\n",
    "most_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.423293Z",
     "iopub.status.busy": "2026-02-19T01:23:25.422772Z",
     "iopub.status.idle": "2026-02-19T01:23:25.433111Z",
     "shell.execute_reply": "2026-02-19T01:23:25.433111Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "552f3c8660d614763a474cee85c9f82c",
     "grade": true,
     "grade_id": "cell-f47caca092487d0a",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'most_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmost_unique\u001b[49m) == \u001b[32m59\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m most_unique[\u001b[32m0\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33marticle\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m most_unique[-\u001b[32m1\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mstory\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'most_unique' is not defined"
     ]
    }
   ],
   "source": [
    "assert len(most_unique) == 59\n",
    "assert most_unique[0] == 'article'\n",
    "assert most_unique[-1] == 'story'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6d1c7d62467a109e67ff0a6fccd8754",
     "grade": false,
     "grade_id": "cell-0b2d1c19fc1ed79d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Take a look through this list of most unique words over time. Do they make sense based on what you know about American history? Do any surprise you?\n",
    "\n",
    "With that part of our Analysis done, one thing that stuck out to me in this list is the fact that \"british\" was the most unique word to the 1813 inaugural address. This made sense to me - it was early in American history and we had only recently left British rule. But, I was curious to see whether or not 'british' would show up uniquely (albeit less uniquely) in any later addresses. **Generate a line plot that plots the TF-IDF for the word \"british\" on the y-axis. Plot year on the x-axis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.434620Z",
     "iopub.status.busy": "2026-02-19T01:23:25.434620Z",
     "iopub.status.idle": "2026-02-19T01:23:25.439148Z",
     "shell.execute_reply": "2026-02-19T01:23:25.439148Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d03145bb4088c2085768cdbabf2e5391",
     "grade": false,
     "grade_id": "cell-bf04bf8b6004eb21",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "f1 = plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.440152Z",
     "iopub.status.busy": "2026-02-19T01:23:25.440152Z",
     "iopub.status.idle": "2026-02-19T01:23:25.456109Z",
     "shell.execute_reply": "2026-02-19T01:23:25.456109Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23088b3f0d5c266101c8ad9d4783a880",
     "grade": true,
     "grade_id": "cell-460b35a6a74775fe",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m f1.gca().has_data()\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "assert f1.gca().has_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "950171e70e9b5fce54f0041cfeb23551",
     "grade": false,
     "grade_id": "cell-dbbe87869aea0ea8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Here you should see that over time \"british\" peaked in inaugural addresses at a few interesting points throughout history. What about some other words?\n",
    "\n",
    "Using a similar approach, **plot TF-IDF for \"british\", \"america\", \"war\", and \"jobs\". Take a look at the trends over time. Feel free to look at other words' trends over time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.457400Z",
     "iopub.status.busy": "2026-02-19T01:23:25.457400Z",
     "iopub.status.idle": "2026-02-19T01:23:25.469718Z",
     "shell.execute_reply": "2026-02-19T01:23:25.469212Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a662c01c1d494f7109b8994bf3883f04",
     "grade": false,
     "grade_id": "cell-17185d3b85abfe7f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[32m      4\u001b[39m f2 = plt.gcf()\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "f2 = plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.471227Z",
     "iopub.status.busy": "2026-02-19T01:23:25.470224Z",
     "iopub.status.idle": "2026-02-19T01:23:25.481580Z",
     "shell.execute_reply": "2026-02-19T01:23:25.481580Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77cd0bfe5d9f4f95baab913c3a21f447",
     "grade": true,
     "grade_id": "cell-d091e3a6715de758",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mf2\u001b[49m.gca().has_data()\n",
      "\u001b[31mNameError\u001b[39m: name 'f2' is not defined"
     ]
    }
   ],
   "source": [
    "assert f2.gca().has_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17f7a751c0fe97e42704323102f66e2c",
     "grade": false,
     "grade_id": "cell-5ed15dc929006d89",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You should see that the mention of \"america\" happened frequently in the country's infancy, but then became less common, whereas \"british was really common early on and \"jobs\" has really only become applicable in recent innaugural addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b74d82bd5e64ce04e360472179a6e97",
     "grade": false,
     "grade_id": "cell-be11e4ed9fa207f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As with all analysis, TF-IDF is not without its limitations. Let's take a look at how our results change if we change the `max_features` result in our analysis above to include 4000 words (rather than 2000). **Redo the analysis to 1) calculate TF-IDF for these 4000 words, 2) identify the word with the highest TF-IDF in each year (assignt his to `most_unique_4000`, and 3) generate a dataframe with the most common word from each analysis.Then, take a look to see how changing one argument in your analysis can affect your results! Finally, you can regenerate line plots if you're interseted to see how your plots have changed in this new analysis.**\n",
    "\n",
    "Define a tfidfvectorizer object as `tfidf2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.483090Z",
     "iopub.status.busy": "2026-02-19T01:23:25.482585Z",
     "iopub.status.idle": "2026-02-19T01:23:25.493296Z",
     "shell.execute_reply": "2026-02-19T01:23:25.492782Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6c3a3100b402b2112b20abadb291f66",
     "grade": false,
     "grade_id": "cell-c3903c7bbcbecea6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m tfidf2 = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# define tfidfvectorizer object as tfidf2\n",
    "tfidf2 = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.493806Z",
     "iopub.status.busy": "2026-02-19T01:23:25.493806Z",
     "iopub.status.idle": "2026-02-19T01:23:25.506428Z",
     "shell.execute_reply": "2026-02-19T01:23:25.505868Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6ca47721c21b47f4bbe76b3aec9a942",
     "grade": true,
     "grade_id": "cell-c354c4f19be1ed99",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'max_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mtfidf2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_features\u001b[49m == \u001b[32m4000\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'max_features'"
     ]
    }
   ],
   "source": [
    "assert tfidf2.max_features == 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.506939Z",
     "iopub.status.busy": "2026-02-19T01:23:25.506939Z",
     "iopub.status.idle": "2026-02-19T01:23:25.517769Z",
     "shell.execute_reply": "2026-02-19T01:23:25.517266Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a304c7f05e224e290c4f66f79f4fc248",
     "grade": false,
     "grade_id": "cell-325533e85301c4df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# calculate TF-DF on input text\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# calculate TF-DF on input text\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.518278Z",
     "iopub.status.busy": "2026-02-19T01:23:25.518278Z",
     "iopub.status.idle": "2026-02-19T01:23:25.528558Z",
     "shell.execute_reply": "2026-02-19T01:23:25.528558Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c5b830427ce04d967f0949bfe126a9b",
     "grade": true,
     "grade_id": "cell-5134c4af688c0ec8",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inaug_tfidf2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43minaug_tfidf2\u001b[49m.index) == years\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m inaug_tfidf2.shape == (\u001b[32m59\u001b[39m,\u001b[32m4000\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'inaug_tfidf2' is not defined"
     ]
    }
   ],
   "source": [
    "assert list(inaug_tfidf2.index) == years\n",
    "assert inaug_tfidf2.shape == (59,4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.530072Z",
     "iopub.status.busy": "2026-02-19T01:23:25.530072Z",
     "iopub.status.idle": "2026-02-19T01:23:25.539815Z",
     "shell.execute_reply": "2026-02-19T01:23:25.539815Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05ee3ddd697912d87f277c09a051a407",
     "grade": false,
     "grade_id": "cell-ad72899d05312a9a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# identify most uniuqe word each year from new model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# identify most uniuqe word each year from new model\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.540820Z",
     "iopub.status.busy": "2026-02-19T01:23:25.540820Z",
     "iopub.status.idle": "2026-02-19T01:23:25.552485Z",
     "shell.execute_reply": "2026-02-19T01:23:25.551974Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9573eb33b360f6d6a2f5fcf4f28fc255",
     "grade": true,
     "grade_id": "cell-445917d4a4c19e84",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'most_unique_4000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmost_unique_4000\u001b[49m) == \u001b[32m59\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m most_unique_4000[\u001b[32m0\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mimmutable\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m most_unique_4000[-\u001b[32m1\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mvirus\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'most_unique_4000' is not defined"
     ]
    }
   ],
   "source": [
    "assert len(most_unique_4000) == 59\n",
    "assert most_unique_4000[0] == 'immutable'\n",
    "assert most_unique_4000[-1] == 'virus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Results\n",
    "Join `most_unique` from the original model with this new list `most_unique_4000`\n",
    "in a single dataframe to compare word each year. Name this dataframe `df_unique`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.552994Z",
     "iopub.status.busy": "2026-02-19T01:23:25.552994Z",
     "iopub.status.idle": "2026-02-19T01:23:25.563544Z",
     "shell.execute_reply": "2026-02-19T01:23:25.563033Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb9d40e20f3a3365809b8f783503cbac",
     "grade": false,
     "grade_id": "cell-0e45e7cb67f8de7d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m df_unique = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df_unique = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.564549Z",
     "iopub.status.busy": "2026-02-19T01:23:25.564549Z",
     "iopub.status.idle": "2026-02-19T01:23:25.575803Z",
     "shell.execute_reply": "2026-02-19T01:23:25.575299Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "655d9fb642e5ba0a76d763d7c5762e72",
     "grade": true,
     "grade_id": "cell-97e0b7f8b1f149f4",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mdf_unique\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m == (\u001b[32m59\u001b[39m,\u001b[32m2\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(df_unique.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (df_unique[\u001b[32m0\u001b[39m] == most_unique)[i] \u001b[38;5;129;01mor\u001b[39;00m (df_unique[\u001b[32m1\u001b[39m] == most_unique)[i]\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "assert df_unique.shape == (59,2)\n",
    "\n",
    "for i in range(df_unique.shape[0]):\n",
    "    assert (df_unique[0] == most_unique)[i] or (df_unique[1] == most_unique)[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.576317Z",
     "iopub.status.busy": "2026-02-19T01:23:25.576317Z",
     "iopub.status.idle": "2026-02-19T01:23:25.586545Z",
     "shell.execute_reply": "2026-02-19T01:23:25.586545Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1526b38d1d69ec7205b1ea0653ba1581",
     "grade": false,
     "grade_id": "cell-cfdb2a2308535396",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# regenerate plot\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[32m      5\u001b[39m f3 = plt.gcf()\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# regenerate plot\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "f3 = plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2026-02-19T01:23:25.587551Z",
     "iopub.status.busy": "2026-02-19T01:23:25.587551Z",
     "iopub.status.idle": "2026-02-19T01:23:25.599441Z",
     "shell.execute_reply": "2026-02-19T01:23:25.598928Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee7faab590134cd4b6cdca32ce9c7640",
     "grade": true,
     "grade_id": "cell-67b714ab1e979f55",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mf3\u001b[49m.gca().has_data()\n",
      "\u001b[31mNameError\u001b[39m: name 'f3' is not defined"
     ]
    }
   ],
   "source": [
    "assert f3.gca().has_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36a33d717455cd8fa2b33d2186f57493",
     "grade": false,
     "grade_id": "cell-1f1c57aaa1e86ead",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Good work getting comfortable working with text data here...and hopefully learning a bit more about Inaugural Addresses over time. Go ahead and submit your discussion lab!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
